const Clarifai = require('clarifai')

// Takes the return Object of a Clarifai model
// Outputs into a Object of <concept name> : <concept value> pairs
// Currently only works with the "Food" and "Moderation" models
// modelType is preliminarily included as a parameter in case adjustments have to be made for additional models
const toConcepts = function(modelOutput, modelType) {
    return modelOutput["data"]["concepts"]
        .reduce((acc, conceptObj) => Object.assign(acc, {
            [conceptObj["name"]]: conceptObj["value"]
        }), {})
}

// IMAGE TYPE CONSTANTS
// SHOULD POTENTIALLY BE DEFINED ELSEWHERE AND IMPORTED HERE
// FOR TESTING PURPOSES
const IMAGES_FOOD = "FoodPorn"
const IMAGES_BEARS = "bears"
const IMAGES_CARS = "carporn"
const IMAGES_DOGS = "dogpictures"
const IMAGES_CATS = "cat"
const IMAGES_PLANTS = "plants"

// MODEL TYPE CONSTANTS
// SHOULD POTENTIALLY BE DEFINED ELSEWHERE AND IMPORTED HERE
// FOR TESTING PURPOSES
const MODEL_MODERATION = Clarifai.MODERATION_MODEL
const MODEL_FOOD = Clarifai.FOOD_MODEL

// Takes the Model Type and Image Type
// Also takes outputs of Clarifai models and images
// The ordering of arrays modelOutputs and images should correspond to each other
// Generates:
//  A Question for a game round based on the Model and Image types
//  Scores for all of the images
//
module.exports.generatePromptAndScores = function ({
    modelType,
    imageType,
    modelOutputs,
    images
}) {
    const conceptsArr = modelOutputs.map(output => toConcepts(output, modelType))

    let promptString = "Which "
    switch(imageType) {
        case IMAGES_FOOD: promptString += partialQuestion_image_food()
            break
        case IMAGES_BEARS: promptString += partialQuestion_image_bears()
            break
        case IMAGES_CARS: promptString += partialQuestion_image_cars()
            break
        case IMAGES_DOGS: promptString += partialQuestion_image_dogs()
            break
        case IMAGES_CATS: promptString += partialQuestion_image_cats()
            break
        case IMAGES_PLANTS: promptString += partialQuestion_image_plants()
            break
        default:
            throw new Error("Error: undefined image type in generatePromptAndScores")
    }
    promptString += " is "

    let additionalInfo = null
    switch(modelType) {
        case MODEL_FOOD: additionalInfo = choice([
            partialQuestion_and_Score_model_food_sum_max,
            partialQuestion_and_Score_model_food_sum_min
        ])(conceptsArr)
            break
        case MODEL_MODERATION: additionalInfo = choice([
            partialQuestion_and_Score_model_moderation_max,
            partialQuestion_and_Score_model_moderation_min
        ])(conceptsArr)
            break
        default:
            throw new Error("Error: undefined model type in generatePromptAndScores")
    }

    const {outputs, partialQuestion, reason} = additionalInfo
    promptString += partialQuestion + "?"
    return {
        outputs: outputs
            .map((output, index) => 
                Object.assign({}, output, {image: images[index]})),
        promptString,
        reason
    }
}

// Returns a random choice from an array
const choice = array => array[Math.floor(Math.random() * array.length)]

/**
 * Each partial question generated by image type must work in the context:
 *  Which <partial question> is [...]?
 */

const partialQuestion_image_food = () => choice([
    "meal", "snacc", "alternative life partner",
    "hedonistic delight", "collection of nutrients",
    "cheeky 4am snack", "regional speciality",
    "reason to die just a little bit earlier",
    "modern analogue of prey", "speed-eating challenge"
])

const partialQuestion_image_bears = () => choice([
    "bear", "large danger fluff", "destructive friend",
    "hibernator", "reason to work on your cardio",
    "salmon connoisseur", "alternative dog",
    "member of the Russian cavalry", "actual bear",
    "stinkyboi", "Baloo"
])

const partialQuestion_image_cars = () => choice([
    "car", "boat on wheels", "ground-plane", "box'n'wheels",
    "vroom vrOOM", "environmentalist nightmare", 
    "horse replacer", "petrolium addict", "goes-fast-box",
    "superduperdoublebike", "(if you're creative) mobile housing unit"
])

const partialQuestion_image_dogs = () => choice([
    "dog", "friend", "BEST friend", "dogi", "wonderfull furball",
    "woofer", "doge", "barkbox", "Fido", "tail chaser",
    "tail wagger", "butt sniffer"
])

const partialQuestion_image_cats = () => choice([
    "cat", "angry fur", "tiny apex predator", "purr factory",
    "claw-and-teeth", "self-proclaimed God", "meow machine",
    "cold-blooded murderer", "furniture scratcher", "mouse chaser"
])

const partialQuestion_image_plants = () => choice([
    "plant", "vegan meal", "oxygen generator", "sunlight consumer",
    "thirstyboi", "greenfriend", "shrubbery", "living decoration",
    "firephobiac", "government spy", "relative of trees"
])

/**
 * Score for each image model output is generated by one of seceral functions depending on model
 * These functions always and only get the model concepts as input
 * These concepts are contained in an array; one object of concepts for each image
 * These functions should return Objects containing the original concepts, 
 *  their scores for the game round, and whether or not they are the correct answer
 * These functions should also return a partial question
 * These partial questions must work in the context:
 *  Which <partial question based on image type> is <partial question based on model type and score>?
 * These functions should also return a short motivation "why" the correct answer is correct
 *  This may or may not be used in further development
 */

 // For maximum sum of concepts
const maxSumHelper = function(conceptsArr, excludeKey = null) {
    const scoresArr = conceptsArr
        // Sum all concepts
        .map(concepts => Object.keys(concepts)
            // Option to exclude a key
            .reduce((acc, key) => key === excludeKey ? acc : acc + concepts[key], 0))
    const maxScore = Math.max(...scoresArr)
    const maxIndex = scoresArr.findIndex(score => score === maxScore)

    const outputs = conceptsArr.map((concepts, index) => ({
        concepts,
        score: scoresArr[index],
        correctAnswer: index === maxIndex
    }))
    return outputs
}

// For minimum sum of concepts
const minSumHelper = function(conceptsArr, excludeKey = null) {
    const scoresArr = conceptsArr
        // Sum all concepts
        .map(concepts => Object.keys(concepts)
            // Option to exclude a key
            .reduce((acc, key) => key === excludeKey ? acc : acc + concepts[key], 0))
    const minScore = Math.min(...scoresArr)
    const minIndex = scoresArr.findIndex(score => score === minScore)

    const outputs = conceptsArr.map((concepts, index) => ({
        concepts,
        score: scoresArr[index],
        correctAnswer: index === minIndex
    }))
    return outputs
}

const partialQuestion_and_Score_model_food_sum_max = function(conceptsArr) {
    const outputs = maxSumHelper(conceptsArr)

    const partialQuestion = choice([
        "the greatest meal", "the most filling", "the best mukbang-video material",
        "the most dangerously depravedly delicious", "the best deal for $10 in a restaurant",
        "a good reason to suspend your diet", "a worthy culinary challenge"
    ])

    const reason = choice([
        "Because the computers say there's lots of food in it!",
        "According to leading experts, it would be the tastiest.",
        "Look, we don't argue with the system. We get punished if we do."
    ])

    return {
        outputs,
        partialQuestion,
        reason
    }
}

const partialQuestion_and_Score_model_food_sum_min = function(conceptsArr) {
    const outputs = minSumHelper(conceptsArr)

    const partialQuestion = choice([
        "a meal fit for no one", "terribly unfilling to eat", "a reason to go to McDonken",
        "a bad restaurant experience", "a 1-star Yelp review", 
        "in the next episode of Gordon Ramsey's Kitchen Nightmares",
        "just like grandma used to make: kinda gross (but you love her, she tries so hard)"
    ])

    const reason = choice([
        "If our estimations are correct, there's barely a calorie in there.",
        "I tasted it, it was gross. >:(",
        "Go ahead, take a bite. Prove me wrong. I dare you."
    ])

    return {
        outputs,
        partialQuestion,
        reason
    }
}

const partialQuestion_and_Score_model_moderation_max = function(conceptsArr) {
    const outputs = maxSumHelper(conceptsArr, "safe")

    const partialQuestion = choice([
        "a jolly good time", "a bit more PG13 than the rest", "probably illegal or immoral",
        "why you got kicked out of your local place of worship",
        "what all the cool kids are doing", "a dissapointment to your parents",
        "available online - preferrably in Incognito Mode", "the reason you're going to hell"
    ])

    const reason = choice([
        "Karen really insisted we don't even include it in the game.",
        "The computer found nudity, drugs, gore... You know, the fun stuff.",
        "If you gotta ask, you're not old enough."
    ])

    return {
        outputs,
        partialQuestion,
        reason
    }
}

const partialQuestion_and_Score_model_moderation_min = function(conceptsArr) {
    const outputs = minSumHelper(conceptsArr, "safe")

    const partialQuestion = choice([
        "the least fun at a party", "tame enough to be in a Disney movie",
        "lame as a corpse (but not as cool)", "an appropriate gift for a five year old",
        "the most safe for work", "about as exciting as a dictionary"
    ])

    const reason = choice([
        "The computer called it 'safe', basically. Does that sound fun?",
        "Free of all the good stuff: the stuff that's immoral, illegal, and/or unhealthy.",
        "We asked local religious leaders, and they had no issue with it."
    ])

    return {
        outputs,
        partialQuestion,
        reason
    }
}